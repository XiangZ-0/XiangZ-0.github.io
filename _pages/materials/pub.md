# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv</div><img src='_pages/materials/imgs/BetterDepth.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot Monocular Depth Estimation](https://arxiv.org/pdf/2407.17952)

**<u>Xiang Zhang</u>**, Bingxin Ke, Hayko Riemenschneider, Nando Metzger, Anton Obukhov, Markus Gross<sup>‚Ä†</sup>, Konrad Schindler, Christopher Schroers<sup>‚Ä†</sup>

 \[[arXiv](https://arxiv.org/pdf/2407.17952)\] 
- We propose BetterDepth to boost zero-shot MDE methods with plug-and-play diffusion refiners, achieving robust affine-invariant MDE performance with fine-grained details.
- We design global pre-alignment and local patch masking strategies to enable learning detail refinement from small-scale synthetic datasets while preserving rich prior knowledge from pre-trained MDE models for zero-shot transfer.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024 - Oral</div><img src='_pages/materials/imgs/HiTSR.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[HiT-SR: Hierarchical Transformer for Efficient Image Super-Resolution](https://1drv.ms/b/c/de821e161e64ce08/EVsrOr1-PFFMsXxiRHEmKeoBSH6DPkTuN2GRmEYsl9bvDQ?e=f9wGUO)

**<u>Xiang Zhang</u>**, Yulun Zhang, Fisher Yu

 \[[Code](https://github.com/XiangZ-0/HiT-SR)\] \[[Supp](https://1drv.ms/b/c/de821e161e64ce08/EYmRy-QOjPdFsMRT_ElKQqABYzoIIfDtkt9hofZ5YY_GjQ?e=2Iapqf)\] 
- We propose a simple yet effective strategy (HiT-SR) to convert popular transformer-based SR methods to our hierarchical transformers, boosting SR performance by exploiting multi-scale features and long-range dependencies.
- We design a spatial-channel correlation method to efficiently leverage spatial and channel features with linear computational complexity to window sizes, enabling utilization of large hierarchical windows, e.g., $64\times64$ windows.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='_pages/materials/imgs/GEM.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Generalizing Event-Based Motion Deblurring in Real-World Scenarios](https://arxiv.org/pdf/2308.05932.pdf)

**<u>Xiang Zhang</u>**, Lei Yu<sup>‚Ä†</sup>, Wen Yang, Jianzhuang Liu, Gui-Song Xia

 \[[Code](https://github.com/XiangZ-0/GEM)\] \[[Dataset](https://onedrive.live.com/redir?resid=DE821E161E64CE08!1507&authkey=!AMyYhDNrRw9pZkA&ithint=folder&e=acFSD4)\] \[[Youtube](https://www.youtube.com/watch?v=Otme-tIWfAs)\]
- A scale-aware network is designed to allow flexible setups of input spatial resolutions and enable learning from different temporal scales of motion blur.
- A self-supervised learning framework is proposed for model training with real-world data and performance generalization in spatial and temporal domains.
- A multi-scale real-world blurry dataset (MS-RBD) is constructed to facilitate the evaluation of deblurring performance in real-world scenarios.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2022</div><img src='_pages/materials/imgs/ESAI.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Learning to See Through with Events](https://ieeexplore.ieee.org/document/9973388)

Lei Yu<sup>‚Ä†</sup>, **<u>Xiang Zhang</u>**, Wei Liao, Wen Yang, Gui-Song Xia

 \[[Code](https://github.com/dvs-whu/E-SAI)\] \[[Dataset](https://drive.google.com/drive/folders/1JVA06QYaQwG88BcAIJwjUGjyItR_UDjC)\] \[[Bilibili](https://www.bilibili.com/video/BV1JL411M7n5/?spm_id_from=333.999.0.0&vd_source=019e4ae5ab90b4c54207081152e55aae)\]
- An event-based synthetic aperture imaging (E-SAI) algorithm is proposed to see through dense occlusions even under extreme lighting conditions. 
- A hybrid network composed of an spiking encoder and a convolutional decoder is designed to mitigate the disturbances from occlusions and guarantee the overall reconstruction performance.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2022</div><img src='_pages/materials/imgs/EVDI.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Unifying Motion Deblurring and Frame Interpolation with Events](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Unifying_Motion_Deblurring_and_Frame_Interpolation_With_Events_CVPR_2022_paper.pdf)

**<u>Xiang Zhang</u>**, Lei Yu<sup>‚Ä†</sup>

\[[Code](https://github.com/XiangZ-0/EVDI)\] \[[Youtube](https://www.youtube.com/watch?v=ih7o5PawSCw&t=1s)\]
- We present a unified framework for event-based video deblurring and interpolation (EVDI). 
- By utilizing the constraints between cross-modal frames and events, a fully self-supervised learning method is proposed to enable network training with real-world data without requiring ground-truth images.

</div>
</div>

- `TPAMI 2024` [CrossZoom: Simultaneous Motion Deblurring and Event Super-Resolving](https://arxiv.org/pdf/2309.16949), Chi Zhang, **<u>Xiang Zhang</u>**, Mingyuan Lin, Cheng Li, Chu He, Wen Yang, Gui-Song Xia, Lei Yu. \| \[[Website](https://bestrivenzc.github.io/CZ-Net/)\]

- `TIP 2024` [Neuromorphic Synergy for Video Binarization](https://ieeexplore.ieee.org/document/10438401), Shijie Lin, **<u>Xiang Zhang</u>**, Lei Yang, Lei Yu, Bin Zhou, Xiaowei Luo, Wenping Wang, Jia Pan. \| \[[Code&Dataset](https://github.com/eleboss/EBR)\] \[[Youtube](https://www.youtube.com/watch?v=fbdvowAHkn0)\] \[[Bilibili](https://www.bilibili.com/video/BV1ws4y1X7Sh/?vd_source=019e4ae5ab90b4c54207081152e55aae)\]

- `TPAMI 2023` [Learning to Super-Resolve Blurry Images with Events](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10029887), Lei Yu<sup>‚Ä†</sup>, Bishan Wang, **<u>Xiang Zhang</u>**, Haijian Zhang, Wen Yang, Jianzhuang Liu, Gui-Song Xia. \| \[[Code](https://github.com/ShinyWang33/eSL-Net-Plusplus)\]

- `TSP 2022` [Spiking Sparse Recovery with Non-convex Penalties](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10007054), **<u>Xiang Zhang</u>**, Lei Yu<sup>‚Ä†</sup>, Gang Zheng, Yonina C. Eldar.

- `CVPR 2022` [Synthetic Aperture Imaging with Events and Frames](https://openaccess.thecvf.com/content/CVPR2022/papers/Liao_Synthetic_Aperture_Imaging_With_Events_and_Frames_CVPR_2022_paper.pdf), Wei Liao<sup>\*</sup>, **<u>Xiang Zhang</u>**<sup>\*</sup>, Lei Yu<sup>‚Ä†</sup>, Shijie Lin, Wen Yang, Ning Qiao. \| \[[Code](https://github.com/smjsc/EF-SAI)\] \[[Dataset](https://onedrive.live.com/?authkey=%21AMvAPOnuudsYx1I&id=7ABD0A750B262518%214850&cid=7ABD0A750B262518)\]

- `CVPR 2021` [Event-based Synthetic Aperture Imaging with a Hybrid Network](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Event-Based_Synthetic_Aperture_Imaging_With_a_Hybrid_Network_CVPR_2021_paper.pdf), **<u>Xiang Zhang</u>**<sup>\*</sup>, Wei Liao<sup>\*</sup>, Lei Yu<sup>‚Ä†</sup>, Wen Yang, Gui-Song Xia. <font color='red'> (Oral, Best Paper Candidate) </font> \| \[[Code](https://github.com/dvs-whu/E-SAI)\] \[[Dataset](https://drive.google.com/drive/folders/1JVA06QYaQwG88BcAIJwjUGjyItR_UDjC)\] \[[Youtube](https://www.youtube.com/watch?v=a81xBe2ZX_8)\]  

<sup>\*</sup> means equal contribution and <sup>‚Ä†</sup> indicates my supervisor.


<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

**Kaiming He**, Xiangyu Zhang, Shaoqing Ren, Jian Sun

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>

- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->